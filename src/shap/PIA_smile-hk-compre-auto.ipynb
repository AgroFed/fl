{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e3f1e3e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-93182970c695>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'autoreload'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdlib\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfunctools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmultiprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mProcess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dlib'"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import asyncio, copy, cv2, dlib, os, socket, sys, time\n",
    "from functools import partial\n",
    "from multiprocessing import Pool, Process\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "import shap\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"../../\")))\n",
    "from libs import nn, resnet, sim, helper\n",
    "#from cfgs.fedargs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5350131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device settings\n",
    "use_cuda = torch.cuda.is_available()\n",
    "torch.manual_seed(10)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "kwargs = {\"num_workers\": 1, \"pin_memory\": True} if use_cuda else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb43bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shap_plot(explain, img, size):\n",
    "    #shap_image = g_img.unsqueeze(0)\n",
    "    shap_values = explain.shap_values(img.unsqueeze(0))\n",
    "    \n",
    "    shap_tensor = torch.tensor(shap_values)\n",
    "    shap_lists = []\n",
    "    for i in range(2):\n",
    "        lsum = 0\n",
    "        lst = shap_tensor[i].squeeze(0).squeeze(0).view(size).tolist()\n",
    "        shap_lists.append(lst)\n",
    "        for l in lst:\n",
    "            if l > 0:\n",
    "                lsum += abs(l)\n",
    "        print ('shap_sum', lsum)\n",
    "    \n",
    "    shap_numpy = [np.swapaxes(np.swapaxes(s, 1, -1), 1, 2) for s in shap_values]\n",
    "    test_numpy = np.swapaxes(np.swapaxes(img.unsqueeze(0).numpy(), 1, -1), 1, 2)\n",
    "    shap.image_plot(shap_numpy, -test_numpy, None, 20, 0.2, 0.2, None, False)\n",
    "    #shap.image_plot(shap_values, -shap_image)\n",
    "    return shap_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec7c9eb",
   "metadata": {},
   "source": [
    "<h1>Gender: ResNet</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f1f7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/celeba/list_attr_celeba.txt', sep=\"\\s+\", skiprows=1, usecols=['Male', 'Smiling'])\n",
    "\n",
    "df1 = df[df['Smiling'] == 1]\n",
    "\n",
    "df1 = df1[['Male']]\n",
    "print(len(df), len(df1))\n",
    "\n",
    "# Make 0 (female) & 1 (male) labels instead of -1 & 1\n",
    "df1.loc[df1['Male'] == -1, 'Male'] = 0\n",
    "\n",
    "df1.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8401aa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('../../data/celeba/list_eval_partition.txt', sep=\"\\s+\", skiprows=0, header=None)\n",
    "df2.columns = ['Filename', 'Partition']\n",
    "df2 = df2.set_index('Filename')\n",
    "\n",
    "df2.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16422ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df1.merge(df2, left_index=True, right_index=True)\n",
    "df3.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3bfa62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.to_csv('celeba-gender-partitions.csv')\n",
    "df4 = pd.read_csv('celeba-gender-partitions.csv', index_col=0)\n",
    "df4.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ff4b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.loc[df4['Partition'] == 0].to_csv('celeba-gender-train.csv')\n",
    "df4.loc[df4['Partition'] == 1].to_csv('celeba-gender-valid.csv')\n",
    "df4.loc[df4['Partition'] == 2].to_csv('celeba-gender-test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b821bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('../../data/celeba/img_align_celeba/000001.jpg')\n",
    "print(np.asarray(img, dtype=np.uint8).shape)\n",
    "plt.imshow(img);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec554ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CelebaDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for loading CelebA face images\"\"\"\n",
    "\n",
    "    def __init__(self, csv_path, img_dir, transform=None):\n",
    "    \n",
    "        df = pd.read_csv(csv_path, index_col=0)\n",
    "        self.img_dir = img_dir\n",
    "        self.csv_path = csv_path\n",
    "        self.img_names = df.index.values\n",
    "        self.y = df['Male'].values\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(os.path.join(self.img_dir,\n",
    "                                      self.img_names[index]))\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        label = self.y[index]\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a83be24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that transforms.ToTensor()\n",
    "# already divides pixels by 255. internally\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "custom_transform = transforms.Compose([transforms.CenterCrop((178, 178)),\n",
    "                                       transforms.Resize((128, 128)),\n",
    "                                       #transforms.Grayscale(),                                       \n",
    "                                       #transforms.Lambda(lambda x: x/255.),\n",
    "                                       transforms.ToTensor()])\n",
    "\n",
    "train_dataset = CelebaDataset(csv_path='celeba-gender-train.csv',\n",
    "                              img_dir='../../data/celeba/img_align_celeba/',\n",
    "                              transform=custom_transform)\n",
    "\n",
    "valid_dataset = CelebaDataset(csv_path='celeba-gender-valid.csv',\n",
    "                              img_dir='../../data/celeba/img_align_celeba/',\n",
    "                              transform=custom_transform)\n",
    "\n",
    "test_dataset = CelebaDataset(csv_path='celeba-gender-test.csv',\n",
    "                             img_dir='../../data/celeba/img_align_celeba/',\n",
    "                             transform=custom_transform)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          shuffle=True,\n",
    "                          num_workers=4)\n",
    "\n",
    "valid_loader = DataLoader(dataset=valid_dataset,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          shuffle=False,\n",
    "                          num_workers=4)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                         batch_size=BATCH_SIZE,\n",
    "                         shuffle=False,\n",
    "                         num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec355b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/celeba/list_attr_celeba.txt', sep=\"\\s+\", skiprows=1, usecols=['Male', 'Smiling'])\n",
    "df.loc[df['Male'] == -1, 'Male'] = 0\n",
    "\n",
    "male_s = df[(df.Male==1) & (df.Smiling == 1)][:10]\n",
    "male_ns = df[(df.Male==1) & (df.Smiling == -1)][:10]\n",
    "female_s = df[(df.Male==0) & (df.Smiling == 1)][:10]\n",
    "female_ns = df[(df.Male==0) & (df.Smiling == -1)][:10]\n",
    "\n",
    "male_s.to_csv('male_s.csv')\n",
    "male_ns.to_csv('male_ns.csv')\n",
    "female_s.to_csv('female_s.csv')\n",
    "female_ns.to_csv('female_ns.csv')\n",
    "\n",
    "train_male_s_dataset = CelebaDataset(csv_path='male_s.csv',\n",
    "                              img_dir='../../data/celeba/img_align_celeba/',\n",
    "                              transform=custom_transform)\n",
    "\n",
    "train_male_ns_dataset = CelebaDataset(csv_path='male_ns.csv',\n",
    "                              img_dir='../../data/celeba/img_align_celeba/',\n",
    "                              transform=custom_transform)\n",
    "\n",
    "train_female_s_dataset = CelebaDataset(csv_path='female_s.csv',\n",
    "                              img_dir='../../data/celeba/img_align_celeba/',\n",
    "                              transform=custom_transform)\n",
    "\n",
    "train_female_ns_dataset = CelebaDataset(csv_path='female_ns.csv',\n",
    "                              img_dir='../../data/celeba/img_align_celeba/',\n",
    "                              transform=custom_transform)\n",
    "\n",
    "\n",
    "train_male_s_loader = DataLoader(dataset=train_male_s_dataset,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          shuffle=True,\n",
    "                          num_workers=4)\n",
    "\n",
    "train_male_ns_loader = DataLoader(dataset=train_male_ns_dataset,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          shuffle=True,\n",
    "                          num_workers=4)\n",
    "\n",
    "train_female_s_loader = DataLoader(dataset=train_female_s_dataset,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          shuffle=True,\n",
    "                          num_workers=4)\n",
    "\n",
    "train_female_ns_loader = DataLoader(dataset=train_female_ns_dataset,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          shuffle=True,\n",
    "                          num_workers=4)\n",
    "\n",
    "target_fns_batch = next(iter(train_female_ns_loader))\n",
    "target_fns_images, _ = target_fns_batch\n",
    "\n",
    "target_fs_batch = next(iter(train_female_s_loader))\n",
    "target_fs_images, _ = target_fs_batch\n",
    "\n",
    "target_mns_batch = next(iter(train_male_ns_loader))\n",
    "target_mns_images, _ = target_mns_batch\n",
    "\n",
    "target_ms_batch = next(iter(train_male_s_loader))\n",
    "target_ms_images, _ = target_ms_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4476fb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model = resnet.ResNet18(2)\n",
    "#MS_FS\n",
    "target_model.load_state_dict(torch.load(\"../../out/models/PIA_smile/TM(MS30K_FS_30K)_3e_pth\", map_location ='cpu'))\n",
    "#MNS_FNS\n",
    "#target_model.load_state_dict(torch.load(\"../../out/models/PIA_smile/TM(MNS_30K_FNS_30K)_pth\", map_location ='cpu'))\n",
    "#MNS_FS\n",
    "#target_model.load_state_dict(torch.load(\"../../out/models/PIA_smile/TM(MNS_30K_FS_30K)_pth\", map_location ='cpu'))\n",
    "#MS_FNS\n",
    "#target_model.load_state_dict(torch.load(\"../../out/models/PIA_smile/TM(MS_30K_FNS_30K)_pth\", map_location ='cpu'))\n",
    "#MS_30_MNS_10_FS_30_FNS_10\n",
    "#target_model.load_state_dict(torch.load(\"../../out/models/PIA_result/TM(MS_30K_MNS_10K_FS_30K_FNS_10K)_pth\", map_location ='cpu'))\n",
    "#MS_10_MNS_30_FS_10_FNS_30\n",
    "#target_model.load_state_dict(torch.load(\"../../out/models/PIA_HK/TM(MS_30K_FNS_30K)_3E_HK.pth\", map_location ='cpu'))\n",
    "target_model.eval()\n",
    "print(\"Model Loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa91bc96",
   "metadata": {},
   "source": [
    "<h1>SHAP Explain</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a602f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_image(tensor):\n",
    "    tensor = tensor*255\n",
    "    tensor = np.array(tensor, dtype=np.uint8)\n",
    "    if np.ndim(tensor)>3:\n",
    "        assert tensor.shape[0] == 1\n",
    "        tensor = tensor[0]\n",
    "    return PIL.Image.fromarray(tensor)\n",
    "\n",
    "target_batch = next(iter(test_loader))\n",
    "target_images, target_labels = target_batch\n",
    "target_baseline = target_images\n",
    "\n",
    "#smiling_female_images = [2,14,26,28,33,35,37,41,42,44]\n",
    "#non_smiling_female_images = [3,4,10,12,15,50,61,63,95,99]\n",
    "\n",
    "#smiling_male_images = [5,19,23,24,27,29,32,40,46,49,54]\n",
    "#non_smiling_male_images = [51,53,62,97]\n",
    "    \n",
    "target_explain = shap.DeepExplainer(target_model, target_baseline)\n",
    "target_image = target_images[2]\n",
    "plt.imshow(target_image.numpy()[0])\n",
    "print(target_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2129a620",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_img = torch.tensor([[[0.0 for col in range(128)] for row in range(128)] for dim in range(3)])\n",
    "r_img = target_image\n",
    "#z_img = z_img.reshape(1,28,28)\n",
    "r_img.shape\n",
    "#plt.imshow(z_img.numpy().squeeze())\n",
    "plt.imshow(target_image[0:3, 80:120, 40:80].numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f39fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_shap_values = shap_plot(target_explain, r_img, 49152)\n",
    "target_female = torch.tensor(target_shap_values[0]).view(-1) \n",
    "target_male = torch.tensor(target_shap_values[1]).view(-1)\n",
    "\n",
    "###\n",
    "target_female_crop = torch.tensor(target_shap_values[0][0:1, 0:3, 80:120, 40:80]).view(-1) \n",
    "target_male_crop = torch.tensor(target_shap_values[1][0:1, 0:3, 80:120, 40:80]).view(-1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0460f46",
   "metadata": {},
   "source": [
    "<h1>PIA</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b0b784",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_female_smile_crop = target_female_crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f13c8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = sum(target_female_smile_crop - target_female_crop)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4facac",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_male_smile_crop = target_male_crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21add111",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = sum(target_male_smile_crop - target_male_crop)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077e0652",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_explain = shap.DeepExplainer(target_model, target_baseline)\n",
    "\n",
    "for smile_female_img in range(2,10):\n",
    "    \n",
    "    target_image = target_fs_images[smile_female_img]\n",
    "    r_img = target_image\n",
    "    target_shap_values = shap_plot(target_explain, r_img, 49152)\n",
    "    target_female = torch.tensor(target_shap_values[0]).view(-1) \n",
    "    #target_male = torch.tensor(target_shap_values[1]).view(-1)\n",
    "    \n",
    "    target_smile_female_crop = torch.tensor(target_shap_values[0][0:1, 0:3, 80:120, 40:80]).view(-1) \n",
    "    #target_male_crop = torch.tensor(target_shap_values[1][0:1, 0:3, 80:120, 40:80]).view(-1) \n",
    "    \n",
    "    for non_smile_female_img in range(10):\n",
    "        target_image = target_fns_images[non_smile_female_img]\n",
    "        r_img = target_image\n",
    "        target_shap_values = shap_plot(target_explain, r_img, 49152)\n",
    "        target_female = torch.tensor(target_shap_values[0]).view(-1) \n",
    "        #target_male = torch.tensor(target_shap_values[1]).view(-1)\n",
    "\n",
    "        target_non_smile_female_crop = torch.tensor(target_shap_values[0][0:1, 0:3, 80:120, 40:80]).view(-1) \n",
    "        #target_male_crop = torch.tensor(target_shap_values[1][0:1, 0:3, 80:120, 40:80]).view(-1) \n",
    "        \n",
    "        res = sum(target_non_smile_female_crop - target_smile_female_crop)\n",
    "        print(res)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26552fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_explain = shap.DeepExplainer(target_model, target_baseline)\n",
    "\n",
    "for smile_male_img in range(2,10):\n",
    "    \n",
    "    target_image = target_ms_images[smile_male_img]\n",
    "    r_img = target_image\n",
    "    target_shap_values = shap_plot(target_explain, r_img, 49152)\n",
    "    #target_male = torch.tensor(target_shap_values[0]).view(-1) \n",
    "    target_male = torch.tensor(target_shap_values[1]).view(-1)\n",
    "    \n",
    "    #target_smile_male_crop = torch.tensor(target_shap_values[0][0:1, 0:3, 80:120, 40:80]).view(-1) \n",
    "    target_smile_male_crop = torch.tensor(target_shap_values[1][0:1, 0:3, 80:120, 40:80]).view(-1) \n",
    "    \n",
    "    for non_smile_male_img in range(10):\n",
    "        target_image = target_mns_images[non_smile_male_img]\n",
    "        r_img = target_image\n",
    "        target_shap_values = shap_plot(target_explain, r_img, 49152)\n",
    "        #target_male = torch.tensor(target_shap_values[0]).view(-1) \n",
    "        target_male = torch.tensor(target_shap_values[1]).view(-1)\n",
    "\n",
    "        #target_non_smile_male_crop = torch.tensor(target_shap_values[0][0:1, 0:3, 80:120, 40:80]).view(-1) \n",
    "        target_non_smile_male_crop = torch.tensor(target_shap_values[1][0:1, 0:3, 80:120, 40:80]).view(-1) \n",
    "        \n",
    "        res = sum(target_non_smile_male_crop - target_smile_male_crop)\n",
    "        print(res)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b993651",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:syft]",
   "language": "python",
   "name": "conda-env-syft-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
