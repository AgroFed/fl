{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30d6b8d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hkasyap/anaconda3/envs/menv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import os, sys\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"../../\")))\n",
    "from libs import data as dt, neuronshap as ns, sim\n",
    "from cfgs.fedargs import *\n",
    "\n",
    "from fairlearn.metrics import (\n",
    "    demographic_parity_difference,\n",
    "    demographic_parity_ratio,\n",
    "    equalized_odds_difference,\n",
    "    equalized_odds_ratio,\n",
    "    false_negative_rate,\n",
    "    false_positive_rate,\n",
    "    true_negative_rate,\n",
    "    true_positive_rate,\n",
    ")\n",
    "from libs.helpers.finance import bin_hours_per_week\n",
    "from libs.helpers.metrics import (\n",
    "    conditional_demographic_parity_difference,\n",
    "    conditional_demographic_parity_ratio,\n",
    ")\n",
    "from libs.helpers.plot import group_box_plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03742d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\n",
    "    \"age\",\n",
    "    \"workclass\",\n",
    "    \"fnlwgt\",\n",
    "    \"education\",\n",
    "    \"education_num\",\n",
    "    \"marital_status\",\n",
    "    \"occupation\",\n",
    "    \"relationship\",\n",
    "    \"race\",\n",
    "    \"sex\",\n",
    "    \"capital_gain\",\n",
    "    \"capital_loss\",\n",
    "    \"hours_per_week\",\n",
    "    \"native_country\",\n",
    "    \"salary\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48829768",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(s):\n",
    "    \"\"\"\n",
    "    Helper function that strips leading / trailing whitespace, lower\n",
    "    cases, and replaces hyphens with underscores.\n",
    "    \"\"\"\n",
    "    return s.strip().lower().replace(\"-\", \"_\")\n",
    "\n",
    "\n",
    "def parse_native_country(country):\n",
    "    \"\"\"\n",
    "    Group countries other than United-States and Mexico into single\n",
    "    \"other\" category\"\n",
    "    \"\"\"\n",
    "    country = clean_string(country)\n",
    "    if country == \"united_states\" or country == \"mexico\":\n",
    "        return country\n",
    "    return \"other\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c65649ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = (\n",
    "    pd.read_csv(\n",
    "        \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\",\n",
    "        header=None,\n",
    "        na_values=[\" ?\"],\n",
    "        names=names,\n",
    "    )\n",
    "    .drop(columns=[\"fnlwgt\", \"education_num\"])\n",
    "    # drop all rows with missing values\n",
    "    .dropna()\n",
    "    .reset_index(drop=True)\n",
    "    # simple preprocessing on columns\n",
    "    .assign(\n",
    "        # clean all string columns\n",
    "        education=lambda df: df.education.map(clean_string),\n",
    "        marital_status=lambda df: df.marital_status.map(clean_string),\n",
    "        occupation=lambda df: df.occupation.map(clean_string),\n",
    "        race=lambda df: df.race.map(clean_string),\n",
    "        relationship=lambda df: df.relationship.map(clean_string),\n",
    "        workclass=lambda df: df.workclass.map(clean_string),\n",
    "        # clean and aggregate native_country\n",
    "        native_country=lambda df: df.native_country.map(parse_native_country),\n",
    "        # encode binary features as integers\n",
    "        salary=lambda df: (df.salary == \" >50K\").astype(np.int32),\n",
    "        sex=lambda df: (df.sex == \" Male\").astype(np.int32),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6de67203",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = (\n",
    "    pd.read_csv(\n",
    "        \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\",\n",
    "        header=None,\n",
    "        na_values=[\" ?\"],\n",
    "        skiprows=1,\n",
    "        names=names,\n",
    "    )\n",
    "    .drop(columns=[\"fnlwgt\", \"education_num\"])\n",
    "    # drop all rows with missing values\n",
    "    .dropna()\n",
    "    .reset_index(drop=True)\n",
    "    # simple preprocessing on columns\n",
    "    .assign(\n",
    "        # clean all string columns\n",
    "        education=lambda df: df.education.map(clean_string),\n",
    "        marital_status=lambda df: df.marital_status.map(clean_string),\n",
    "        occupation=lambda df: df.occupation.map(clean_string),\n",
    "        race=lambda df: df.race.map(clean_string),\n",
    "        relationship=lambda df: df.relationship.map(clean_string),\n",
    "        workclass=lambda df: df.workclass.map(clean_string),\n",
    "        # clean and aggregate native_country\n",
    "        native_country=lambda df: df.native_country.map(parse_native_country),\n",
    "        # encode binary features as integers\n",
    "        # note extra '.' in test set not present in train set\n",
    "        salary=lambda df: (df.salary == \" >50K.\").astype(np.int32),\n",
    "        sex=lambda df: (df.sex == \" Male\").astype(np.int32),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d409ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert set(train.education) == set(test.education)\n",
    "assert set(train.race) == set(test.race)\n",
    "assert set(train.relationship) == set(test.relationship)\n",
    "assert set(train.marital_status) == set(test.marital_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6002c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_features = [\n",
    "    \"workclass\",\n",
    "    \"education\",\n",
    "    \"occupation\",\n",
    "    \"race\",\n",
    "    \"relationship\",\n",
    "    \"marital_status\",\n",
    "    \"native_country\",\n",
    "]\n",
    "\n",
    "cts_features = [\"age\", \"capital_gain\", \"capital_loss\", \"hours_per_week\"]\n",
    "\n",
    "binary_features = [\"sex\", \"salary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10bb52f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "white                 25933\n",
       "black                  2817\n",
       "asian_pac_islander      895\n",
       "amer_indian_eskimo      286\n",
       "other                   231\n",
       "Name: race, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"race\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1daaad08",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat(\n",
    "    [train, pd.get_dummies(train.loc[:, one_hot_features], dtype=np.int32)],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "test_df = pd.concat(\n",
    "    [test, pd.get_dummies(test.loc[:, one_hot_features], dtype=np.int32)],\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ac757cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert train_df.columns.tolist() == test_df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e364942",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f4ec8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../../data/adult\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "050908ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_features = cts_features + one_hot_features + binary_features\n",
    "\n",
    "train_df[original_features].to_csv(\"../../data/adult/train.csv\", index=False)\n",
    "val_df[original_features].to_csv(\"../../data/adult/val.csv\", index=False)\n",
    "test_df[original_features].to_csv(\"../../data/adult/test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ff4c660",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "\n",
    "train_df[cts_features] = ss.fit_transform(train_df[cts_features])\n",
    "val_df[cts_features] = ss.transform(val_df[cts_features])\n",
    "test_df[cts_features] = ss.transform(test_df[cts_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46215377",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop(columns=one_hot_features).to_csv(\"../../data/adult/train-one-hot.csv\", index=False)\n",
    "val_df.drop(columns=one_hot_features).to_csv(\"../../data/adult/val-one-hot.csv\", index=False)\n",
    "test_df.drop(columns=one_hot_features).to_csv(\"../../data/adult/test-one-hot.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "400948bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../../data/adult/train.csv\")\n",
    "val = pd.read_csv(\"../../data/adult/val.csv\")\n",
    "test = pd.read_csv(\"../../data/adult/test.csv\")\n",
    "\n",
    "train_oh = pd.read_csv(\"../../data/adult/train-one-hot.csv\")\n",
    "val_oh = pd.read_csv(\"../../data/adult/val-one-hot.csv\")\n",
    "test_oh = pd.read_csv(\"../../data/adult/test-one-hot.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c98258b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital_gain</th>\n",
       "      <th>capital_loss</th>\n",
       "      <th>hours_per_week</th>\n",
       "      <th>salary</th>\n",
       "      <th>workclass_federal_gov</th>\n",
       "      <th>workclass_local_gov</th>\n",
       "      <th>workclass_private</th>\n",
       "      <th>workclass_self_emp_inc</th>\n",
       "      <th>...</th>\n",
       "      <th>marital_status_divorced</th>\n",
       "      <th>marital_status_married_af_spouse</th>\n",
       "      <th>marital_status_married_civ_spouse</th>\n",
       "      <th>marital_status_married_spouse_absent</th>\n",
       "      <th>marital_status_never_married</th>\n",
       "      <th>marital_status_separated</th>\n",
       "      <th>marital_status_widowed</th>\n",
       "      <th>native_country_mexico</th>\n",
       "      <th>native_country_other</th>\n",
       "      <th>native_country_united_states</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.015917</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.147741</td>\n",
       "      <td>-0.218133</td>\n",
       "      <td>-0.079269</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.029378</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.147741</td>\n",
       "      <td>-0.218133</td>\n",
       "      <td>0.752765</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.788255</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.147741</td>\n",
       "      <td>-0.218133</td>\n",
       "      <td>-0.079269</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.425948</td>\n",
       "      <td>1</td>\n",
       "      <td>0.872159</td>\n",
       "      <td>-0.218133</td>\n",
       "      <td>-0.079269</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.332929</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.147741</td>\n",
       "      <td>-0.218133</td>\n",
       "      <td>-0.911303</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        age  sex  capital_gain  capital_loss  hours_per_week  salary  \\\n",
       "0 -1.015917    1     -0.147741     -0.218133       -0.079269       0   \n",
       "1 -0.029378    1     -0.147741     -0.218133        0.752765       0   \n",
       "2 -0.788255    1     -0.147741     -0.218133       -0.079269       1   \n",
       "3  0.425948    1      0.872159     -0.218133       -0.079269       1   \n",
       "4 -0.332929    1     -0.147741     -0.218133       -0.911303       0   \n",
       "\n",
       "   workclass_federal_gov  workclass_local_gov  workclass_private  \\\n",
       "0                      0                    0                  1   \n",
       "1                      0                    0                  1   \n",
       "2                      0                    1                  0   \n",
       "3                      0                    0                  1   \n",
       "4                      0                    0                  1   \n",
       "\n",
       "   workclass_self_emp_inc  ...  marital_status_divorced  \\\n",
       "0                       0  ...                        0   \n",
       "1                       0  ...                        0   \n",
       "2                       0  ...                        0   \n",
       "3                       0  ...                        0   \n",
       "4                       0  ...                        0   \n",
       "\n",
       "   marital_status_married_af_spouse  marital_status_married_civ_spouse  \\\n",
       "0                                 0                                  0   \n",
       "1                                 0                                  1   \n",
       "2                                 0                                  1   \n",
       "3                                 0                                  1   \n",
       "4                                 0                                  0   \n",
       "\n",
       "   marital_status_married_spouse_absent  marital_status_never_married  \\\n",
       "0                                     0                             1   \n",
       "1                                     0                             0   \n",
       "2                                     0                             0   \n",
       "3                                     0                             0   \n",
       "4                                     0                             1   \n",
       "\n",
       "   marital_status_separated  marital_status_widowed  native_country_mexico  \\\n",
       "0                         0                       0                      0   \n",
       "1                         0                       0                      0   \n",
       "2                         0                       0                      0   \n",
       "3                         0                       0                      0   \n",
       "4                         0                       0                      0   \n",
       "\n",
       "   native_country_other  native_country_united_states  \n",
       "0                     0                             1  \n",
       "1                     0                             1  \n",
       "2                     0                             1  \n",
       "3                     0                             1  \n",
       "4                     0                             1  \n",
       "\n",
       "[5 rows x 64 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_oh.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "904c7af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/ritvikkhanna09/Census-classifier-comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd2c28b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "race_amer_indian_eskimo               15060 non-null  int64  \n",
    "race_asian_pac_islander               15060 non-null  int64  \n",
    "race_black                            15060 non-null  int64  \n",
    "race_other                            15060 non-null  int64  \n",
    "race_white\n",
    " \n",
    "'''\n",
    "\n",
    "mr_dh_oh = test_oh.loc[(test_oh[\"race_asian_pac_islander\"] == 1) | (test_oh[\"race_white\"] == 1)]\n",
    "mr_dh_oh = mr_dh_oh.head(100)\n",
    "fmr_dh_oh = test_oh.loc[(test_oh[\"race_amer_indian_eskimo\"] == 1) | (test_oh[\"race_black\"] == 1) | (test_oh[\"race_other\"] == 1)]\n",
    "fmr_dh_oh = fmr_dh_oh.head(100)\n",
    "\n",
    "\n",
    "m_dh_oh = test_oh.loc[test_oh[\"sex\"] == 1]\n",
    "m_dh_oh = m_dh_oh.head(100)\n",
    "fm_dh_oh = test_oh.loc[test_oh[\"sex\"] == 0]\n",
    "fm_dh_oh = fm_dh_oh.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c535017e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_oh.drop(columns=\"salary\").values\n",
    "Y_train = train_oh['salary'].values\n",
    "X_test = test_oh.drop(columns=\"salary\").values\n",
    "Y_test = test_oh['salary'].values\n",
    "X_m = m_dh_oh.drop(columns=\"salary\").values\n",
    "Y_m = m_dh_oh['salary'].values\n",
    "X_fm = fm_dh_oh.drop(columns=\"salary\").values\n",
    "Y_fm = fm_dh_oh['salary'].values\n",
    "\n",
    "#creating torch dataset and loader using original dataset. \n",
    "#to use resampled dataset, replace ex. xtrain with xtrain_over etc.\n",
    "train_data = torch.utils.data.TensorDataset(torch.tensor(X_train).float(), torch.tensor(Y_train).long())\n",
    "test_data = torch.utils.data.TensorDataset(torch.tensor(X_test).float(), torch.tensor(Y_test).long())\n",
    "m_data = torch.utils.data.TensorDataset(torch.tensor(X_m).float(), torch.tensor(Y_m).long())\n",
    "fm_data = torch.utils.data.TensorDataset(torch.tensor(X_fm).float(), torch.tensor(Y_fm).long())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data,batch_size=128, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=len(test_data))\n",
    "m_loader = torch.utils.data.DataLoader(m_data, batch_size=1)\n",
    "fm_loader = torch.utils.data.DataLoader(fm_data, batch_size=1)\n",
    "mr_loader = torch.utils.data.DataLoader(m_data, batch_size=1)\n",
    "fmr_loader = torch.utils.data.DataLoader(fm_data, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3e242062",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicNet(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super().__init__()\n",
    "        self.num_features = num_features\n",
    "        self.num_classes = num_classes\n",
    "        self.layers = 0\n",
    "        \n",
    "        self.lin1 = torch.nn.Linear(self.num_features,  150)        \n",
    "        self.lin2 = torch.nn.Linear(50, 50)        \n",
    "        self.lin3 = torch.nn.Linear(50, 50)\n",
    "        \n",
    "        self.lin4 = torch.nn.Linear(150, 150) \n",
    "        \n",
    "        self.lin5 = torch.nn.Linear(50, 50)        \n",
    "        self.lin6 = torch.nn.Linear(50, 50)\n",
    "        self.lin10 = torch.nn.Linear(150, self.num_classes)\n",
    "        \n",
    "        self.prelu = torch.nn.PReLU()\n",
    "        self.dropout = torch.nn.Dropout(0.25)\n",
    "\n",
    "    def forward(self, xin):\n",
    "        self.layers = 0\n",
    "        \n",
    "        x = F.relu(self.lin1(xin))\n",
    "        self.layers += 1\n",
    "        \n",
    "        #x = F.relu(self.lin2(x))\n",
    "        #self.layers += 1\n",
    "        for y in range(8):\n",
    "            x = F.relu(self.lin4(x)) \n",
    "            self.layers += 1\n",
    "           \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = F.relu(self.lin10(x)) \n",
    "        self.layers += 1\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0f7df169",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    \n",
    "    for inputs, target in train_loader:\n",
    "      \n",
    "        #inputs, target = inputs.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(inputs)\n",
    "        loss = loss_fn(output, target.long())\n",
    "        # Backprop\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eac1d0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    \n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    test_size = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "      \n",
    "        for inputs, target in test_loader:\n",
    "            \n",
    "            #inputs, target = inputs.to(device), target.to(device)\n",
    "            \n",
    "            output = model(inputs)\n",
    "            test_size += len(inputs)\n",
    "            test_loss += test_loss_fn(output, target.long()).item() \n",
    "            pred = output.max(1, keepdim=True)[1] \n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= test_size\n",
    "    accuracy = correct / test_size\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, test_size,\n",
    "        100. * accuracy))\n",
    "    \n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b3540472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training beginning...\n",
      "Epoch  1 :\n",
      "\n",
      "Test set: Average loss: 0.3240, Accuracy: 12780/15060 (85%)\n",
      "\n",
      "Epoch  2 :\n",
      "\n",
      "Test set: Average loss: 0.3444, Accuracy: 12662/15060 (84%)\n",
      "\n",
      "Epoch  3 :\n",
      "\n",
      "Test set: Average loss: 0.3283, Accuracy: 12571/15060 (83%)\n",
      "\n",
      "Epoch  4 :\n",
      "\n",
      "Test set: Average loss: 0.3218, Accuracy: 12703/15060 (84%)\n",
      "\n",
      "Epoch  5 :\n",
      "\n",
      "Test set: Average loss: 0.3326, Accuracy: 12749/15060 (85%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = BasicNet(63, 2)\n",
    "test_accuracy = []\n",
    "train_loss = []\n",
    "nbr_epochs = 5\n",
    "lr = 0.0025# \n",
    "weight_decay = 0\n",
    "\n",
    "# Surrogate loss used for training\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "test_loss_fn = torch.nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr,weight_decay=weight_decay)\n",
    "#optimizer = optim.SGD(model.parameters(), lr=lr ,weight_decay=weight_decay)\n",
    "#optimizer = optim.RMSprop(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "print('Training beginning...')\n",
    "#start_time = time.time()\n",
    "\n",
    "for epoch in range(1, nbr_epochs+1):\n",
    "    print('Epoch ', epoch, ':')\n",
    "    train(model, train_loader, optimizer, epoch)\n",
    "    loss, acc = test(model, test_loader)\n",
    "    \n",
    "    # save results every epoch\n",
    "    test_accuracy.append(acc)\n",
    "    train_loss.append(loss)\n",
    "    \n",
    "#end_time = time.time()\n",
    "#print('Training on ' + str(nbr_epochs) + ' epochs done in ', str(end_time-start_time),' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5cca14a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 12749/15060 (85%)\n",
      "\n",
      "3700 tensor([3645]) tensor([3645])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for inputs, target in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        pred = outputs.max(1, keepdim=True)[1] \n",
    "        correct = pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "        accuracy = correct / len(inputs)\n",
    "        print('\\nAccuracy: {}/{} ({:.0f}%)\\n'.format(correct, len(inputs), 100. * accuracy))\n",
    "\n",
    "Y_prob = F.softmax(outputs, dim=1)[:, 1]\n",
    "Y_pred = outputs.max(1, keepdim=True)[1]\n",
    "\n",
    "print(sum(Y_test), sum(Y_pred), sum(pred))\n",
    "\n",
    "test = pd.read_csv(\"../../data/adult/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f221b5",
   "metadata": {},
   "source": [
    "<h1>Demographic Parity</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de150a4",
   "metadata": {},
   "source": [
    "<h2>Distribution of scores by sex</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "eae1acc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demographic parity difference: 0.071\n",
      "Demographic parity ratio: 0.366\n"
     ]
    }
   ],
   "source": [
    "dpd = demographic_parity_difference(\n",
    "    Y_test, Y_pred, sensitive_features=test.sex,\n",
    ")\n",
    "dpr = demographic_parity_ratio(\n",
    "    Y_test, Y_pred, sensitive_features=test.sex,\n",
    ")\n",
    "\n",
    "print(f\"Demographic parity difference: {dpd:.3f}\")\n",
    "print(f\"Demographic parity ratio: {dpr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bd99e4",
   "metadata": {},
   "source": [
    "<h2>Distribution of scores by race</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "93c5c319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demographic parity difference: 0.114\n",
      "Demographic parity ratio: 0.105\n"
     ]
    }
   ],
   "source": [
    "dpd = demographic_parity_difference(\n",
    "    Y_test, Y_pred, sensitive_features=test.race,\n",
    ")\n",
    "dpr = demographic_parity_ratio(\n",
    "    Y_test, Y_pred, sensitive_features=test.race,\n",
    ")\n",
    "\n",
    "print(f\"Demographic parity difference: {dpd:.3f}\")\n",
    "print(f\"Demographic parity ratio: {dpr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6306bd3",
   "metadata": {},
   "source": [
    "<h1>Conditional Demographic Parity</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d5067d",
   "metadata": {},
   "source": [
    "<h2>Distribution of scores by sex and hours worked per week</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8ecb4ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditional demographic parity difference: 0.059\n",
      "Conditional demographic parity ratio: 0.627\n"
     ]
    }
   ],
   "source": [
    "test_hpw_enum = test.hours_per_week.map(bin_hours_per_week)\n",
    "\n",
    "cdpd = conditional_demographic_parity_difference(\n",
    "    Y_test, Y_pred, test.sex, test_hpw_enum,\n",
    ")\n",
    "cdpr = conditional_demographic_parity_ratio(\n",
    "    Y_test, Y_pred, test.sex, test_hpw_enum,\n",
    ")\n",
    "\n",
    "print(f\"Conditional demographic parity difference: {cdpd:.3f}\")\n",
    "print(f\"Conditional demographic parity ratio: {cdpr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5af8b0",
   "metadata": {},
   "source": [
    "<h2>Distribution of scores by race and hours worked per week</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ff666f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditional demographic parity difference: 0.175\n",
      "Conditional demographic parity ratio: 0.058\n"
     ]
    }
   ],
   "source": [
    "cdpd = conditional_demographic_parity_difference(\n",
    "    Y_test, Y_pred, test.race, test_hpw_enum,\n",
    ")\n",
    "cdpr = conditional_demographic_parity_ratio(\n",
    "    Y_test, Y_pred, test.race, test_hpw_enum,\n",
    ")\n",
    "\n",
    "print(f\"Conditional demographic parity difference: {cdpd:.3f}\")\n",
    "print(f\"Conditional demographic parity ratio: {cdpr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540eb7be",
   "metadata": {},
   "source": [
    "<h1>Equalised Odds</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83403a17",
   "metadata": {},
   "source": [
    "<h2>Distribution of scores by sex for high and low earners</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ccb4a352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equalised odds difference: 0.011\n",
      "Equalised odds ratio: 0.319\n"
     ]
    }
   ],
   "source": [
    "eod = equalized_odds_difference(\n",
    "    Y_test, Y_pred, sensitive_features=test.sex,\n",
    ")\n",
    "eor = equalized_odds_ratio(\n",
    "    Y_test, Y_pred, sensitive_features=test.sex,\n",
    ")\n",
    "\n",
    "print(f\"Equalised odds difference: {eod:.3f}\")\n",
    "print(f\"Equalised odds ratio: {eor:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfe14f3",
   "metadata": {},
   "source": [
    "<h2>Distribution of scores by race for high and low earners</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "282afbe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equalised odds difference: 0.250\n",
      "Equalised odds ratio: 0.000\n"
     ]
    }
   ],
   "source": [
    "eod = equalized_odds_difference(\n",
    "    Y_test, Y_pred, sensitive_features=test.race,\n",
    ")\n",
    "eor = equalized_odds_ratio(\n",
    "    Y_test, Y_pred, sensitive_features=test.race,\n",
    ")\n",
    "\n",
    "print(f\"Equalised odds difference: {eod:.3f}\")\n",
    "print(f\"Equalised odds ratio: {eor:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a025501d",
   "metadata": {},
   "source": [
    "<h1>Shapley based Neuron Pruning for Fairness</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "907f2736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  52.877377   77.55837    16.379549 ... 3388.5007   1607.3066\n",
      "    0.      ]\n",
      "[  49.849373    0.         14.680171 ... 3549.225     531.03326\n",
      "    0.      ]\n"
     ]
    }
   ],
   "source": [
    "m_shapley_values = ns.calculate_shapley_values_fa(model, m_loader, 100)\n",
    "print(m_shapley_values)\n",
    "fm_shapley_values = ns.calculate_shapley_values_fa(model, fm_loader, 100)\n",
    "print(fm_shapley_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "66e84c4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  71.62713 ,   71.912186,   71.67966 ,   72.16496 ,   76.35712 ,\n",
       "         73.06438 ,   73.455605,   72.96662 ,   73.79617 ,   76.343445,\n",
       "         72.58319 ,   75.73064 ,   76.61609 ,   96.18156 ,   89.79048 ,\n",
       "         88.41321 ,   81.17374 ,   88.33698 ,   92.27591 ,   76.83716 ,\n",
       "         81.58118 ,   88.52043 ,   99.85916 ,   83.91992 ,   78.1815  ,\n",
       "         88.490036,   90.81383 ,   83.61939 ,   76.74428 ,   77.55837 ,\n",
       "         92.45502 ,   94.560814,   77.05978 ,   88.72458 ,   97.26541 ,\n",
       "         85.29304 ,   85.1465  ,   90.27347 ,  103.81008 ,   93.79936 ,\n",
       "         81.38835 ,   94.92759 ,   95.455826,   90.86333 ,   87.24123 ,\n",
       "         99.99918 ,   86.26289 ,   89.44297 ,   94.014404,   92.83373 ,\n",
       "         91.95735 ,  101.382454,   94.18377 ,   79.73165 ,   81.64512 ,\n",
       "         91.42771 ,   88.085495,   95.64511 ,   85.33082 ,   79.674385,\n",
       "         92.457306,   97.00473 ,   97.52826 ,   93.462715,   78.68754 ,\n",
       "        101.50879 ,   99.74773 ,   80.26223 ,   89.34128 ,   82.681946,\n",
       "         81.14629 ,   92.57727 ,   92.29399 ,   91.15901 ,  101.15983 ,\n",
       "         79.50255 ,   82.80298 ,  104.09498 ,  110.83398 ,  182.01515 ,\n",
       "        107.89137 ,  117.527435,  115.49667 ,  205.7818  ,  171.59317 ,\n",
       "        180.45874 ,  119.560486,  136.66188 ,  143.179   ,  192.93639 ,\n",
       "        152.10786 ,  220.50671 ,  162.14014 ,  167.26308 ,  121.81953 ,\n",
       "        221.22867 ,  168.50551 ,  110.028366,  108.89534 ,  108.08913 ,\n",
       "        116.83421 ,  141.0884  ,  112.775154,  105.54036 ,  133.48672 ,\n",
       "        805.92865 ,  133.22308 ,  154.10739 ,  119.03497 ,  135.53777 ,\n",
       "        131.91383 ,  423.09662 ,  148.06555 ,  115.90197 ,  653.24243 ,\n",
       "        111.05705 ,  182.06647 ,  459.73322 ,  109.06417 ,  121.910416,\n",
       "        131.3817  ,  280.4792  ,  142.1333  ,  122.483826,  169.2785  ,\n",
       "        182.0134  ,  120.80372 ,  111.81775 ,  567.2311  ,  109.589874,\n",
       "        114.61636 ,  105.084816,  136.04333 ,  315.87946 ,  114.766594,\n",
       "        137.27055 ,  201.50856 ,  493.98358 ,  427.097   ,  400.25    ,\n",
       "        111.98811 ,  190.14728 ,  282.12042 ,  191.91327 ,  663.8206  ,\n",
       "        486.3953  ,  152.2002  ,  119.51778 , 1076.2734  ,  526.22314 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_shap_values = m_shapley_values - fm_shapley_values\n",
    "max_diff_shap_values_ind = np.argpartition(diff_shap_values, -150)[-150:]\n",
    "diff_shap_values[max_diff_shap_values_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3373eb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_arr, model_slist = sim.get_net_arr(model)\n",
    "model_arr[max_diff_shap_values_ind] = 0\n",
    "updated_model = sim.get_arr_net(model, model_arr, model_slist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "36e1159b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 12425/15060 (83%)\n",
      "\n",
      "3700 tensor([1343]) tensor([1343])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for inputs, target in test_loader:\n",
    "        outputs = updated_model(inputs)\n",
    "        pred = outputs.max(1, keepdim=True)[1] \n",
    "        correct = pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "        accuracy = correct / len(inputs)\n",
    "        print('\\nAccuracy: {}/{} ({:.0f}%)\\n'.format(correct, len(inputs), 100. * accuracy))\n",
    "        \n",
    "\n",
    "Y_prob = F.softmax(outputs, dim=1)[:, 1]\n",
    "Y_pred = outputs.max(1, keepdim=True)[1]\n",
    "\n",
    "print(sum(Y_test), sum(Y_pred), sum(pred))\n",
    "\n",
    "test = pd.read_csv(\"../../data/adult/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "39c508d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demographic parity difference: 0.071\n",
      "Demographic parity ratio: 0.366\n"
     ]
    }
   ],
   "source": [
    "dpd = demographic_parity_difference(\n",
    "    Y_test, Y_pred, sensitive_features=test.sex,\n",
    ")\n",
    "dpr = demographic_parity_ratio(\n",
    "    Y_test, Y_pred, sensitive_features=test.sex,\n",
    ")\n",
    "\n",
    "print(f\"Demographic parity difference: {dpd:.3f}\")\n",
    "print(f\"Demographic parity ratio: {dpr:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3dc56384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditional demographic parity difference: 0.121\n",
      "Conditional demographic parity ratio: 0.526\n"
     ]
    }
   ],
   "source": [
    "test_hpw_enum = test.hours_per_week.map(bin_hours_per_week)\n",
    "\n",
    "cdpd = conditional_demographic_parity_difference(\n",
    "    Y_test, Y_pred, test.sex, test_hpw_enum,\n",
    ")\n",
    "cdpr = conditional_demographic_parity_ratio(\n",
    "    Y_test, Y_pred, test.sex, test_hpw_enum,\n",
    ")\n",
    "\n",
    "print(f\"Conditional demographic parity difference: {cdpd:.3f}\")\n",
    "print(f\"Conditional demographic parity ratio: {cdpr:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "946bca74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equalised odds difference: 0.046\n",
      "Equalised odds ratio: 0.295\n"
     ]
    }
   ],
   "source": [
    "eod = equalized_odds_difference(\n",
    "    Y_test, Y_pred, sensitive_features=test.sex,\n",
    ")\n",
    "eor = equalized_odds_ratio(\n",
    "    Y_test, Y_pred, sensitive_features=test.sex,\n",
    ")\n",
    "\n",
    "print(f\"Equalised odds difference: {eod:.3f}\")\n",
    "print(f\"Equalised odds ratio: {eor:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3bb6b7d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:menv]",
   "language": "python",
   "name": "conda-env-menv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
