{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70a77f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, sys\n",
    "import copy\n",
    "import socket\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import pickle\n",
    "from torch import optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "#!pip install networkx matplotlib\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"../\")))\n",
    "from libs.topology_manager import *\n",
    "from libs import fl, nn, agg, data, log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c9749cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Logs To File (info | debug | warning | error | critical) [optional]\n",
    "log.init(\"info\")\n",
    "#log.init(\"info\", \"federated.log\")\n",
    "#log.init(\"debug\", \"flkafka.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79199bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_mat = np.array([[0,1,1,1,1,1],\n",
    "             [1,0,0,0,0,0],\n",
    "             [1,0,0,0,0,0],\n",
    "             [1,0,0,0,0,0],\n",
    "             [1,0,0,0,0,0], \n",
    "             [1,0,0,0,0,0]])\n",
    "\n",
    "node_types = {'aggregator': [0,1], 'trainer': [2,3], 'broadcaster': [4,5]}\n",
    "\n",
    "di_graph = nx.DiGraph(adj_mat)\n",
    "#nx.draw(di_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea0623fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FedArgs():\n",
    "    def __init__(self):\n",
    "        self.num_clients = len(adj_mat)\n",
    "        self.epochs = 10\n",
    "        self.local_rounds = 1\n",
    "        self.client_batch_size = 32\n",
    "        self.test_batch_size = 128\n",
    "        self.learning_rate = 1e-4\n",
    "        self.weight_decay = 1e-5\n",
    "        self.cuda = False\n",
    "        self.seed = 1\n",
    "        self.topic = 'pyflx-c'\n",
    "        self.server_topic = \"pyflx-cs\"\n",
    "        self.tb = SummaryWriter('../out/runs/flkafka', comment=\"Mnist Distributed Federated training\")\n",
    "\n",
    "fedargs = FedArgs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a3022ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = fedargs.cuda and torch.cuda.is_available()\n",
    "torch.manual_seed(fedargs.seed)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "kwargs = {\"num_workers\": 1, \"pin_memory\": True} if use_cuda else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e8de969",
   "metadata": {},
   "outputs": [],
   "source": [
    "host = socket.gethostname()\n",
    "server = host + \"(server)\"\n",
    "clients = [host + \"(\" + str(client + 1) + \")\" for client in range(fedargs.num_clients)]\n",
    "ctp = CentralizedTopology(adj_mat, server, clients, node_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1008670f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize Global and Client models\n",
    "global_model = nn.ModelMNIST()\n",
    "client_models = {client: copy.deepcopy(global_model) for client in clients}\n",
    "\n",
    "# Function for training\n",
    "def train_model(_model, train_loader, fedargs, device):\n",
    "    model, loss = fl.client_update(_model,\n",
    "                                train_loader,\n",
    "                                fedargs.learning_rate,\n",
    "                                fedargs.weight_decay,\n",
    "                                fedargs.local_rounds,\n",
    "                                device)\n",
    "    model_update = agg.sub_model(_model, model)\n",
    "    return model_update, model, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7283a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST Data to clients\n",
    "train_data, test_data = data.load_dataset(\"mnist\")\n",
    "clients_data = data.split_data(train_data, clients)\n",
    "client_train_loaders, client_test_loaders = data.load_client_data(clients_data, fedargs.client_batch_size, 0.2, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=fedargs.test_batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "clients_info = {\n",
    "        client: {\"train_loader\": client_train_loaders[client],\n",
    "                 \"test_loader\": client_test_loaders[client]}\n",
    "        for client in clients\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "280fc803",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import time\n",
    "\n",
    "def background(f):\n",
    "    def wrapped(*args, **kwargs):\n",
    "        return asyncio.get_event_loop().run_in_executor(None, f, *args, **kwargs)\n",
    "\n",
    "    return wrapped\n",
    "\n",
    "@background\n",
    "def process(client, epoch, ctp, model, train_loader, test_loader, fedargs, device):\n",
    "    log.info(\"Processing Client {}\".format(client))\n",
    "\n",
    "    # Consume and Average, epoch passed is actually prev epoch, for which we want to consume updates\n",
    "    rcvd_models = ctp.consume_model(client, fedargs.server_topic, model, epoch)\n",
    "    log.info(\"Client {} received {} model update(s) from {}\".format(client, len(rcvd_models), list(rcvd_models.keys())))\n",
    "    if len(rcvd_models) != 0:\n",
    "        model = fl.federated_avg(rcvd_models)\n",
    "\n",
    "    # Train  \n",
    "    model_update, model, loss = train_model(model, train_loader, fedargs, device)\n",
    "    \n",
    "    ctp.produce_model(client, fedargs.topic, model_update, epoch)\n",
    "\n",
    "    # Plot and Log\n",
    "    for local_epoch, loss in enumerate(list(loss.values())):\n",
    "        fedargs.tb.add_scalars(\"Training Loss/\" + client, {str(epoch): loss}, str(local_epoch + 1))\n",
    "\n",
    "    log.jsondebug(loss, \"Epoch {} of {} : Federated Training loss, Client {}\".format(epoch, fedargs.epochs, client))\n",
    "    log.modeldebug(model, \"Epoch {} of {} : Client {} Update\".format(epoch, fedargs.epochs, client))\n",
    "\n",
    "    # Test\n",
    "    test_output = fl.eval(model, test_loader, device)\n",
    "    fedargs.tb.add_scalar(\"Accuracy/\" + client, test_output[\"accuracy\"], epoch)\n",
    "    log.jsoninfo(test_output, \"Test Outut after Epoch {} of {} for Client {}\".format(epoch, fedargs.epochs, client))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc40c00d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]2021-08-18 16:13:54,999 - <ipython-input-20-4eda9d332668>::<module>(l:8) : Federated Training Epoch 1 of 10 [MainProcess : MainThread (INFO)]\n",
      "2021-08-18 16:13:55,353 - /home/harsh_1921cs01/hub/F3IA/fl/libs/protobuf_producer.py::produce(l:56) : Producing user records to topic pyflx-cs. ^C to exit. [MainProcess : MainThread (INFO)]\n",
      "2021-08-18 16:13:55,636 - /home/harsh_1921cs01/hub/F3IA/fl/libs/protobuf_producer.py::produce(l:66) : Flushing records... [MainProcess : MainThread (INFO)]\n",
      "2021-08-18 16:13:56,680 - /home/harsh_1921cs01/hub/F3IA/fl/libs/protobuf_producer.py::delivery_report(l:50) : User record b'bladecluster.iitp.org(server)' successfully produced to pyflx-cs [0] at offset 68 [MainProcess : MainThread (INFO)]\n",
      "2021-08-18 16:13:56,741 - <ipython-input-19-89d53eeba195>::process(l:12) : Processing Client bladecluster.iitp.org(4) [MainProcess : asyncio_0 (INFO)]\n",
      "2021-08-18 16:13:56,765 - <ipython-input-19-89d53eeba195>::process(l:12) : Processing Client bladecluster.iitp.org(2) [MainProcess : asyncio_1 (INFO)]\n",
      "2021-08-18 16:13:56,772 - <ipython-input-19-89d53eeba195>::process(l:12) : Processing Client bladecluster.iitp.org(1) [MainProcess : asyncio_5 (INFO)]\n",
      "2021-08-18 16:13:56,773 - <ipython-input-19-89d53eeba195>::process(l:12) : Processing Client bladecluster.iitp.org(3) [MainProcess : asyncio_2 (INFO)]\n",
      "2021-08-18 16:13:56,785 - <ipython-input-19-89d53eeba195>::process(l:12) : Processing Client bladecluster.iitp.org(6) [MainProcess : asyncio_3 (INFO)]\n",
      "2021-08-18 16:13:56,786 - <ipython-input-19-89d53eeba195>::process(l:12) : Processing Client bladecluster.iitp.org(5) [MainProcess : asyncio_4 (INFO)]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# Federated Training\n",
    "for _epoch in tqdm(range(fedargs.epochs)):\n",
    "\n",
    "    epoch = _epoch + 1\n",
    "    log.info(\"Federated Training Epoch {} of {}\".format(epoch, fedargs.epochs))\n",
    "    \n",
    "    if epoch > 1:\n",
    "        # Server, Consume and Average, epoch passed is actually prev epoch, for which we want to consume updates\n",
    "        client_model_updates = ctp.consume_model(server, fedargs.topic, global_model, epoch - 1)\n",
    "        log.info(\"Server received {} model update(s) from {}\".format(len(client_model_updates), list(client_model_updates.keys())))\n",
    "        if len(client_model_updates) != 0:\n",
    "            global_model = fl.federated_avg(client_model_updates, global_model)\n",
    "        \n",
    "        # Gloabal Test\n",
    "        log.modeldebug(global_model, \"Epoch {} of {} : Server Update\".format(epoch, fedargs.epochs))\n",
    "        global_test_output = fl.eval(global_model, test_loader, device)\n",
    "        fedargs.tb.add_scalar(\"Gloabl Accuracy/\", global_test_output[\"accuracy\"], epoch)\n",
    "        log.jsoninfo(global_test_output, \"Gloabl Test Outut after Epoch {} of {}\".format(epoch, fedargs.epochs))\n",
    "        \n",
    "    # Push Server Update    \n",
    "    ctp.produce_model(server, fedargs.server_topic, global_model, epoch)\n",
    "    \n",
    "    # Clients\n",
    "    tasks = [process(client, epoch, ctp, client_models[client],\n",
    "                     clients_info[client]['train_loader'],\n",
    "                     clients_info[client]['test_loader'],\n",
    "                     fedargs, device) for client in clients]\n",
    "    await asyncio.wait(tasks)\n",
    "\n",
    "print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed52dd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:syft]",
   "language": "python",
   "name": "conda-env-syft-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
